{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qz2o1gj40jN",
        "outputId": "79a61274-1861-43d1-a4a7-25bb293470be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16352 sha256=d876e351fb61656ecedeb6326af39d12455480bc9f371f237fe46419ca3214d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f2/e0/d578821d723b473d18610ea93810e4a5402463919f07e603d9\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.4.0 requires httpx<1.0.0dev,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.13 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk googletrans==3.1.0a0 scikit-learn textblob flask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from googletrans import Translator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from flask import Flask, request, jsonify\n",
        "import threading"
      ],
      "metadata": {
        "id": "sRnh6E5L6Mlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYMEvnRZ6Q0s",
        "outputId": "04930b87-1a2c-4a9d-cb60-e60cc3b4616c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Text Preprocessing Module\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.ps = PorterStemmer()\n",
        "        self.translator = Translator()\n",
        "        self.stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "    def preprocess(self, text, language='en'):\n",
        "        \"\"\"Preprocess text based on language\"\"\"\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        if language == 'en':\n",
        "            filtered = [w for w in tokens if w not in self.stopwords_en]\n",
        "        else:\n",
        "            filtered = tokens  # Add more language-specific stopwords as needed\n",
        "        stemmed = [self.ps.stem(word) for word in filtered]\n",
        "        return ' '.join(stemmed)"
      ],
      "metadata": {
        "id": "KlqfQbfr6Tr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Multilingual Translation Module\n",
        "class LanguageHandler:\n",
        "    def __init__(self):\n",
        "        self.translator = Translator()\n",
        "        self.supported_langs = {'en': 'english', 'mr': 'marathi', 'hi': 'hindi'}\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        \"\"\"Detect input language\"\"\"\n",
        "        return self.translator.detect(text).lang\n",
        "\n",
        "    def translate(self, text, src_lang, dest_lang='en'):\n",
        "        \"\"\"Translate text between languages\"\"\"\n",
        "        if src_lang == dest_lang:\n",
        "            return text\n",
        "        return self.translator.translate(text, src=src_lang, dest=dest_lang).text"
      ],
      "metadata": {
        "id": "mHZS-ZDE6YY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Feature Extraction Module\n",
        "class FeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.tfidf = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "    def extract_features(self, texts):\n",
        "        \"\"\"Convert text to TF-IDF features\"\"\"\n",
        "        return self.tfidf.fit_transform(texts)\n",
        "\n",
        "    def transform(self, texts):\n",
        "        \"\"\"Transform new text using fitted vectorizer\"\"\"\n",
        "        return self.tfidf.transform(texts)"
      ],
      "metadata": {
        "id": "hR2bXogc6a4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Intent Recognition Module\n",
        "class IntentClassifier:\n",
        "    def __init__(self):\n",
        "        self.model = LogisticRegression()\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        \"\"\"Train intent classification model\"\"\"\n",
        "        X = self.feature_extractor.extract_features(texts)\n",
        "        self.model.fit(X, labels)\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"Predict intent for new text\"\"\"\n",
        "        X = self.feature_extractor.transform([text])\n",
        "        return self.model.predict(X)[0]"
      ],
      "metadata": {
        "id": "FkP8Y2cw6dZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Sentiment Analysis Module\n",
        "class SentimentAnalyzer:\n",
        "    def analyze(self, text):\n",
        "        \"\"\"Analyze sentiment and return category\"\"\"\n",
        "        blob = TextBlob(text)\n",
        "        polarity = blob.sentiment.polarity\n",
        "        if polarity > 0:\n",
        "            return 'positive'\n",
        "        elif polarity < 0:\n",
        "            return 'negative'\n",
        "        return 'neutral'"
      ],
      "metadata": {
        "id": "9ObRlsFo6fjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Fake News Detection Module\n",
        "class FakeNewsDetector:\n",
        "    def __init__(self):\n",
        "        self.model = LogisticRegression()\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        \"\"\"Train fake news detection model\"\"\"\n",
        "        X = self.feature_extractor.extract_features(texts)\n",
        "        self.model.fit(X, labels)\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"Predict if text is fake\"\"\"\n",
        "        X = self.feature_extractor.transform([text])\n",
        "        return self.model.predict(X)[0] == 0  # 0 for real, 1 for fake"
      ],
      "metadata": {
        "id": "-8Pw4-x06huF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Main Chatbot Class\n",
        "class MaharashtraChatbot:\n",
        "    def __init__(self):\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.lang_handler = LanguageHandler()\n",
        "        self.intent_classifier = IntentClassifier()\n",
        "        self.sentiment_analyzer = SentimentAnalyzer()\n",
        "        self.fake_detector = FakeNewsDetector()\n",
        "\n",
        "        # Sample knowledge base\n",
        "        self.knowledge_base = {\n",
        "            'schemes': 'Available schemes: 1. Farmer Support 2. Education Grant',\n",
        "            'grievance': 'Please provide details of your grievance',\n",
        "            'services': 'Services: 1. Birth Certificate 2. Tax Payment'\n",
        "        }\n",
        "\n",
        "    def process_input(self, text, user_lang='en'):\n",
        "        \"\"\"Process user input and generate response\"\"\"\n",
        "        detected_lang = self.lang_handler.detect_language(text)\n",
        "        eng_text = self.lang_handler.translate(text, detected_lang, 'en')\n",
        "        processed_text = self.preprocessor.preprocess(eng_text)\n",
        "        sentiment = self.sentiment_analyzer.analyze(eng_text)\n",
        "        intent = self.intent_classifier.predict(processed_text)\n",
        "        is_valid = self.fake_detector.predict(eng_text)\n",
        "\n",
        "        response = self._generate_response(intent, sentiment, is_valid)\n",
        "        final_response = self.lang_handler.translate(response, 'en', user_lang)\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    def _generate_response(self, intent, sentiment, is_valid):\n",
        "        \"\"\"Generate appropriate response\"\"\"\n",
        "        if not is_valid:\n",
        "            return \"Warning: Information could not be verified\"\n",
        "\n",
        "        if intent in self.knowledge_base:\n",
        "            response = self.knowledge_base[intent]\n",
        "            if sentiment == 'negative':\n",
        "                response = f\"I'm sorry to hear you're upset. {response}\"\n",
        "            return response\n",
        "        return \"I'm sorry, I couldn't understand your request. Please try again.\""
      ],
      "metadata": {
        "id": "oEVLmYmP6kEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Flask Application (adapted for Colab)\n",
        "app = Flask(__name__)\n",
        "chatbot = MaharashtraChatbot()"
      ],
      "metadata": {
        "id": "GGQ-8qdB6pXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample training data\n",
        "def initialize_models():\n",
        "    intents = {\n",
        "        'texts': ['government schemes', 'file a complaint', 'available services'],\n",
        "        'labels': ['schemes', 'grievance', 'services']\n",
        "    }\n",
        "    news = {\n",
        "        'texts': ['real government update', 'fake maharashtra news'],\n",
        "        'labels': [0, 1]\n",
        "    }\n",
        "\n",
        "    chatbot.intent_classifier.train(intents['texts'], intents['labels'])\n",
        "    chatbot.fake_detector.train(news['texts'], news['labels'])\n",
        "\n",
        "@app.route('/chatbot', methods=['POST'])\n",
        "def chatbot_endpoint():\n",
        "    data = request.json\n",
        "    user_input = data.get('input', '')\n",
        "    user_lang = data.get('language', 'en')\n",
        "\n",
        "    response = chatbot.process_input(user_input, user_lang)\n",
        "    return jsonify({\n",
        "        'response': response,\n",
        "        'timestamp': pd.Timestamp.now().isoformat()\n",
        "    })"
      ],
      "metadata": {
        "id": "Jev8ZZoR6rlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Function to run Flask in Colab\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=5050)\n",
        "\n",
        "# Main execution in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize models\n",
        "    initialize_models()\n",
        "\n",
        "    # Start Flask in a separate thread since Colab doesn't support direct Flask execution\n",
        "    from threading import Thread\n",
        "    flask_thread = Thread(target=run_flask)\n",
        "    flask_thread.start()\n",
        "\n",
        "    # Test the chatbot directly in Colab\n",
        "    print(\"Testing chatbot directly:\")\n",
        "    test_input = \"What are government schemes?\"\n",
        "    response = chatbot.process_input(test_input, 'en')\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"Response: {response}\")\n",
        "\n",
        "    # Additional test in Marathi\n",
        "    test_input_marathi = \"सरकारी योजना काय आहेत?\"  # \"What are government schemes?\" in Marathi\n",
        "    response_marathi = chatbot.process_input(test_input_marathi, 'mr')\n",
        "    print(f\"\\nInput (Marathi): {test_input_marathi}\")\n",
        "    print(f\"Response (Marathi): {response_marathi}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVzNIPnf6wKO",
        "outputId": "b13b5510-7270-49b0-bd7a-b404e521b64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing chatbot directly:\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5050\n",
            " * Running on http://172.28.0.12:5050\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: What are government schemes?\n",
            "Response: Please provide details of your grievance\n",
            "\n",
            "Input (Marathi): सरकारी योजना काय आहेत?\n",
            "Response (Marathi): कृपया आपल्या तक्रारीचा तपशील प्रदान करा\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(5000)\n",
        "\n",
        "# Then in a new cell, run:\n",
        "import requests\n",
        "import json\n",
        "\n",
        "payload = json.dumps({\"input\": \"What are government schemes?\", \"language\": \"en\"})\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "response = requests.post('http://localhost:5050/chatbot', data=payload, headers=headers)\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "nHfYL_BP6z3p",
        "outputId": "d2281b3a-7907-44c2-9b83-951a7496990c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(5000, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Mar/2025 17:19:40] \"POST /chatbot HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'response': 'Please provide details of your grievance', 'timestamp': '2025-03-19T17:19:40.933780'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Udl3-Kt7n6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}